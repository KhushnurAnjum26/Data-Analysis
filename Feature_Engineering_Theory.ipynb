{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCcH8vOmHr6//liM1RQV+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhushnurAnjum26/Data-Analysis/blob/main/Feature_Engineering_Theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ§  0. Feature Engineering (The Foundation)**\n",
        "\n",
        "\n",
        "Feature Engineering is the process of preparing raw data into features that improve model performance. It's one of the most important tasks in the machine learning pipeline.\n",
        "\n",
        "### **ğŸ” Why It Matters:**\n",
        "A good model with bad features will perform poorly.\n",
        "\n",
        "A simple model with great features can perform extremely well.\n",
        "\n",
        "**ğŸ§° What it includes:**\n",
        "\n",
        "Creating new features\n",
        "\n",
        "Cleaning data\n",
        "\n",
        "Transforming, scaling, and encoding\n",
        "\n",
        "Selecting relevant features\n",
        "\n",
        "## **ğŸ”„ 1. Feature Transformation**\n",
        "\n",
        "Transforming existing features into a more useful format. This helps models understand the data better or handle it more efficiently.\n",
        "\n",
        "# ğŸ“Œ Purpose:\n",
        "\n",
        "Reduce skewness\n",
        "\n",
        "Handle nonlinear relationships\n",
        "\n",
        "Normalize distributions\n",
        "\n",
        "ğŸ”§ Common Techniques:\n",
        "Log transformation:\n",
        "\n",
        "Example: Convert Price = [10, 100, 1000] â†’ log(Price) = [1, 2, 3]\n",
        "\n",
        "# **Power transformations (e.g., square root, cube)**\n",
        "\n",
        "Box-Cox / Yeo-Johnson transformations for normalization\n",
        "\n",
        "# **ğŸš« 1.1. Missing Values Imputation**\n",
        "\n",
        "Missing values (nulls/NaNs) can cause many ML algorithms to fail. You must handle them properly.\n",
        "\n",
        "ğŸ”§ Methods:\n",
        "\n",
        "Mean/Median/Mode:\n",
        "\n",
        "Use mean for continuous, mode for categorical\n",
        "\n",
        "Forward/Backward fill:\n",
        "\n",
        "Use previous or next known value\n",
        "\n",
        "Model-based imputation:\n",
        "\n",
        "Use regression or KNN to estimate missing values\n",
        "\n",
        "## **# ğŸ”¤ 1.2. Handling Categorical Values**\n",
        "\n",
        "Many models can only work with numbers, not strings. Categorical variables must be encoded.\n",
        "\n",
        "ğŸ”§ Methods:\n",
        "\n",
        "Label Encoding: Assigns a number to each category\n",
        "(e.g., Male = 0, Female = 1)\n",
        "\n",
        "One-Hot Encoding: Creates binary columns for each category\n",
        "(e.g., Color = Red, Blue â†’ [1, 0], [0, 1])\n",
        "\n",
        "Target Encoding: Uses the mean of the target variable grouped by category\n",
        "\n",
        "âš ï¸ Tip:\n",
        "Avoid one-hot encoding with too many categories (e.g., Zip Codes).\n",
        "\n",
        "\n",
        "## **ğŸš¨ 1.3. Outlier Detection**\n",
        "\n",
        "Outliers are unusual values that can skew model performance and affect results.\n",
        "\n",
        "ğŸ”§ Methods:\n",
        "IQR Method:\n",
        "\n",
        "OutlierÂ if\n",
        "ğ‘¥\n",
        "<\n",
        "ğ‘„\n",
        "1\n",
        "âˆ’\n",
        "1.5\n",
        "Ã—\n",
        "ğ¼\n",
        "ğ‘„\n",
        "ğ‘…\n",
        "Â or\n",
        "ğ‘¥\n",
        ">\n",
        "ğ‘„\n",
        "3\n",
        "+\n",
        "1.5\n",
        "Ã—\n",
        "ğ¼\n",
        "ğ‘„\n",
        "ğ‘…\n",
        "OutlierÂ ifÂ x<Q1âˆ’1.5Ã—IQRÂ orÂ x>Q3+1.5Ã—IQR\n",
        "Z-score Method:\n",
        "\n",
        "Outliers: Z > 3 or Z < -3\n",
        "\n",
        "Visualization:\n",
        "\n",
        "Boxplots, scatter plots\n",
        "\n",
        "\n",
        "## **âš–ï¸ 1.4. Feature Scaling**\n",
        "\n",
        "Different features have different ranges. Scaling ensures no feature dominates another due to its unit size.\n",
        "\n",
        "ğŸ”§ Methods:\n",
        "Min-Max Scaling (0 to 1):\n",
        "\n",
        "ğ‘¥\n",
        "scaled\n",
        "=\n",
        "ğ‘¥\n",
        "âˆ’\n",
        "min\n",
        "â¡\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "max\n",
        "â¡\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "âˆ’\n",
        "min\n",
        "â¡\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "x\n",
        "scaled\n",
        "â€‹\n",
        " =\n",
        "max(x)âˆ’min(x)\n",
        "xâˆ’min(x)\n",
        "â€‹\n",
        "\n",
        "Standardization (Z-score):\n",
        "\n",
        "ğ‘¥\n",
        "scaled\n",
        "=\n",
        "ğ‘¥\n",
        "âˆ’\n",
        "ğœ‡\n",
        "ğœ\n",
        "x\n",
        "scaled\n",
        "â€‹\n",
        " =\n",
        "Ïƒ\n",
        "xâˆ’Î¼\n",
        "â€‹\n",
        "\n",
        "ğŸ“Œ Why:\n",
        "Essential for algorithms like KNN, SVM, Gradient Descent-based models\n",
        "\n",
        "\n",
        "# **ğŸ§± 3. Feature Construction**\n",
        "\n",
        "Creating new features that better describe the problem to the model, using existing raw data.\n",
        "\n",
        "ğŸ§  Example Ideas:\n",
        "From Date â†’ extract Year, Month, Day\n",
        "\n",
        "Combine Price Ã— Quantity = Total Spend\n",
        "\n",
        "From Text â†’ Count words or calculate sentiment\n",
        "\n",
        "\n",
        "\n",
        "# **âœ… 3. Feature Selection**\n",
        "\n",
        "Reducing the number of features by keeping only the most relevant ones improves speed, reduces overfitting, and improves accuracy.\n",
        "\n",
        "ğŸ”§ Methods:\n",
        "Filter methods: Chi-square, ANOVA, correlation\n",
        "\n",
        "Wrapper methods: Recursive Feature Elimination (RFE)\n",
        "\n",
        "Embedded methods: Lasso Regression (L1), Tree-based models\n",
        "\n",
        "\n",
        "# **ğŸ§¬ 4. Feature Extraction**\n",
        "\n",
        "Transforming original features into a new feature space (usually fewer dimensions) while keeping important information.\n",
        "\n",
        "ğŸ”§ Techniques:\n",
        "\n",
        "PCA (Principal Component Analysis):\n",
        "\n",
        "Reduces dimensionality by creating components that explain max variance\n",
        "\n",
        "LDA (Linear Discriminant Analysis)\n",
        "\n",
        "TF-IDF / Word2Vec: For extracting info from text\n",
        "\n",
        "Autoencoders: Neural network-based compression\n"
      ],
      "metadata": {
        "id": "OQrhX4Xi1P9r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykLpigD5AOam"
      },
      "outputs": [],
      "source": []
    }
  ]
}